<!DOCTYPE html>
<html class="no-js" lang="en">
<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <title>Nicolas Ayobi</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- mobile specific metas
    ================================================== -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/vendor.css">

    <!-- script
    ================================================== -->
    <script defer src="js/vendor/fontawesome/all.min.js"></script>

    <!-- favicons
    ================================================== -->
    <link rel="apple-touch-icon" sizes="180x180" href="icon200.png">
    <link rel="icon" type="image/png" sizes="32x32" href="icon200.png">
    <link rel="icon" type="image/png" sizes="16x16" href="icon200.png">
    <link rel="manifest" href="site.webmanifest">

</head>

<body id="top" class="ss-preload">


    <!-- preloader
    ================================================== -->
    <div id="preloader">
        <div id="loader"></div>
    </div>


    <!-- header
    ================================================== -->
    <header class="s-header">
        <div class="row s-header__nav-wrap">
            <nav class="s-header__nav">
                <ul>
                    <li class="current"><a class="smoothscroll" href="#hero">Home</a></li>
                    <li><a class="smoothscroll" href="#about">About</a></li>
                    <li><a class="smoothscroll" href="#portfolio">Research</a></li>
                    <li><a class="smoothscroll" href="#resume">Experience</a></li>
                    <li><a class="smoothscroll" href="#education">Education</a></li>
                    <!--<li><a class="smoothscroll" href="#skills">Skills</a></li>-->
                    <!--<li><a class="smoothscroll" href="#extracurricular">Extracurriculars</a></li>-->
                </ul>
            </nav>
        </div> <!-- end row -->
        
        <a class="s-header__menu-toggle" href="#0" title="Menu">
            <span class="s-header__menu-icon"></span>
        </a>
    </header> <!-- end s-header -->


    <!-- hero
    ================================================== -->
    <section id="hero" class="s-hero target-section">

        <div class="s-hero__bg rellax" data-rellax-speed="-7"></div>

        <div class="row s-hero__content">
			<img class="s-hero__pic" src="images/main.jpg" alt="">
            <div class="column">

                <div class="s-hero__content-about">

                    <h1>Nicolás Ayobi</h1>

                    <h3 style="text-align: center">
                        <span>AI Engineer</span><br>
                        <span>Research Assistant in Computer Vision</span> at CinfonIA <br>
                        <span>M.Sc., B.Sc in Biomedical Engineering</span><!-- with a double minor in <span>Computational Mathematics</span> and in <span>Bioinformatics</span>-->
                    </h3>

                    <div class="s-hero__content-social">
                        <!--a href="https://www.facebook.com/nicolas.ayobi"><i class="fab fa-facebook-square" aria-hidden="true"></i></a-->
                        <!-- a href="#0"><i class="fab fa-twitter" aria-hidden="true"></i></a-->
                        <!--a href="https://www.instagram.com/nicolasayobi/?hl=es-la"><i class="fab fa-instagram" aria-hidden="true"></i></a-->
                        <!-- a href="#0"><i class="fab fa-dribbble" aria-hidden="true"></i></a-->
                        <!-- a href="#0"><i class="fab fa-behance" aria-hidden="true"></i></a-->
                        <a href="https://www.linkedin.com/in/nicol%C3%A1s-ayobi-mendoza-5b78141b1/" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin" aria-hidden="true"></i></a>
						<a href="https://scholar.google.com/citations?user=YpUSLocAAAAJ&hl=es" target="_blank" rel="noopener noreferrer"><i class="fab fa-google" aria-hidden="true"></i></a>
                        <a href="https://www.researchgate.net/profile/Nicolas-Ayobi" target="_blank" rel="noopener noreferrer"><i class="fab fa-researchgate" aria-hidden="true"></i></a>
                        <!-- <a href="https://orcid.org/0000-0002-0691-3770" target="_blank" rel="noopener noreferrer"></a><i class="fa-brands fa-orcid"></i></a> -->
						<a href="mailto:n.ayobi@uniandes.edu.co" target="_blank" rel="noopener noreferrer"><i class="fas fa-envelope" aria-hidden="true"></i></a>
						<!-- <a href="tel:+573183918489"><i class="fas fa-phone-square-alt" aria-hidden="true"></i></a> -->
                    </div>

                </div> <!-- end s-hero__content-about -->

            </div>
        </div> <!-- s-hero__content -->

        <div class="s-hero__scroll">
            <a href="#about" class="s-hero__scroll-link smoothscroll">
                <span class="scroll-arrow">
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill:rgba(0, 0, 0, 1);"><path d="M18.707 12.707L17.293 11.293 13 15.586 13 6 11 6 11 15.586 6.707 11.293 5.293 12.707 12 19.414z"></path></svg>
                </span>
                <span class="scroll-text">Scroll Down</span>
            </a>
        </div> <!-- s-hero__scroll -->

    </section> <!-- end s-hero -->


    <!-- about
    ================================================== -->
    <section id="about" class="s-about target-section">

        <div class="row">
            <div class="column large-3 tab-12">
                <img class="s-about__pic" src="images/second.jpg" alt="">
            </div>
            <div class="column large-9 tab-12 s-about__content">
                <h3>About Me</h3>
                <p style="text-align: justify;">
                    I work as a research assistant in computer vision at the <a href="https://cinfonia.uniandes.edu.co/" target="_blank" rel="noopener noreferrer">CinfonIA research center</a> 
                    of the <a href="https://uniandes.edu.co/" target="_blank" rel="noopener noreferrer">University of los Andes</a> in Bogota, Colombia under professor's 
                    <a href="https://scholar.google.com/citations?user=k0nZO90AAAAJ&hl=es" target="_blank" rel="noopener noreferrer">Pablo Arbeláez's</a> supervision.
                    I finished my Master's degree in biomedical engineering focused on medical applications of AI in 2024, 
                    and I received my Bachelor's degree in biomedical engineering with a double minor in Bioinformatics and Computational Mathematics in 2022. 
                    During the last four years, I've worked on several research projects focused on deep learning and computer vision for medical and natural images. 
                    My main research interests are image-guided surgical robot intelligence, video analysis, biomedical image processing, image segmentation, and vision & language. 
                </p>

                <hr>

                <div class="row s-about__content-bottom">
                    <div class="column w-1000-stack">
                        <h3>Contact Details</h3>
    
                        <p>
                        <span>City: </span> <a href="https://www.google.com/maps/place/Bogot%C3%A1/@4.6482837,-74.2478938,11z/data=!3m1!4b1!4m5!3m4!1s0x8e3f9bfd2da6cb29:0x239d635520a33914!8m2!3d4.7109886!4d-74.072092" target="_blank" rel="noopener noreferrer">Bogotá D.C., Colombia</a> <br>
                        <span>Zip Code: </span>110121 <br>
                        <!-- <span>Telephone: </span> <a href="tel:+573183918489">+57 318-391-8489</a> <br> -->
                        <span>E-mail: </span> <a href="mailto:n.ayobi@uniandes.edu.co" target="_blank" rel="noopener noreferrer">n.ayobi@uniandes.edu.co</a>
                        </p>
                        
                    </div>
                    <div class="column w-1000-stack">
                        <a href="docs/Nicolas_Ayobi_CV.pdf" class="btn btn--download" target="_blank" rel="noopener noreferrer">
                            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" style="fill:rgba(0, 0, 0, 1);"><path d="M12 16L16 11 13 11 13 4 11 4 11 11 8 11z"></path><path d="M20,18H4v-7H2v7c0,1.103,0.897,2,2,2h16c1.103,0,2-0.897,2-2v-7h-2V18z"></path></svg>
                            Download CV
                        </a>
                    </div>
                </div>
            </div>
        </div> <!-- end row -->

    </section> <!-- end s-about -->

        <!-- portfolio
    ================================================== -->
    <section id="portfolio" class="s-portfolio target-section">

        <div style="margin-bottom: -7%;" class="row s-portfolio__header">
            <div class="column large-12">
                <h3>
                    Publications
                </h3>
                <p>
                    
                </p>
            </div>
        </div>

        <div style="margin-bottom: -55pt;" class="row block-large-full block-medium-1-1 block-tab-1-1 block-500-stack folio-list">
            <div class="column">
                <p style="text-align: center;"> 
                    <em>Find a more detailed report of my publications in my <a href="https://scholar.google.com/citations?user=YpUSLocAAAAJ&hl=en" target="_blank" rel="noopener noreferrer">google scholar</a>, my <a href="https://www.researchgate.net/profile/Nicolas-Ayobi" target="_blank" rel="noopener noreferrer">ResearchGate</a> or my <a href="https://orcid.org/0000-0002-0691-3770" target="_blank" rel="noopener noreferrer">ORCID</a>.</em>
                </p>
            </div>
        </div>

        <div style="margin-bottom: -45pt;" class="row block-large-1-4 block-medium-1-1 block-tab-1-1 block-500-stack folio-list">

            <div class="column folio-item">
                <a href="#modal-08" class="folio-item__thumb">
                    <img src="images/MuST.png" 
                         srcset="images/MuST.png 1x, 
                                 images/MuST.png 2x" 
                         alt=""
                         style="padding-top: 3%; padding-bottom: 3%;"
                                 >
                </a>
            </div> <!-- end folio-item -->

            <div class="column2">
                <p> 
                    <strong>MuST: Multi-Scale Transformers for Surgical Phase Recognition.</strong> <br>
                    Alejandra Pérez, Santiago Rodríguez, <u>Nicolás Ayobi</u>, Nicolás Aparicio, Eugénie Dessevres, and Pablo Arbeláez<br>
                    <em>Poster at the International Conference on Medican Image Computing and Computer Assisted Interventions (MICCAI) 2024.</em><br>
                    
                    <a href="https://arxiv.org/abs/2407.17361" target="_blank" rel="noopener noreferrer">[PDF]</a>
                    <a href=" https://github.com/BCV-Uniandes/MuST" target="_blank" rel="noopener noreferrer">[Code]</a>
                </p>
            </div> <!-- end folio-item -->


            <!-- Modal Templates Popup
            =========================================================== -->
            <div id="modal-08" hidden>
                <div class="modal-popup">
                    <img src="images/MuST.png" alt="" />
        
                    <div class="modal-popup__desc">
                        <!--h5>Panoptic Narrative Grounding</h5-->
                        <p style="text-align: justify">Phase recognition in surgical videos is crucial for enhancing computer-aided surgical systems 
                            as it enables automated understanding of sequential procedural stages. Existing methods often rely on fixed temporal windows 
                            for video analysis to identify dynamic surgical phases. Thus, they struggle to simultaneously capture short-, mid-, and 
                            long-term information necessary to fully understand complex surgical procedures. To address these issues, we propose Multi-Scale 
                            Transformers for Surgical Phase Recognition (MuST), a novel Transformer-based approach that combines a Multi-Term Frame 
                            encoder with a Temporal Consistency Module to capture information across multiple temporal scales of a surgical video. Our 
                            Multi-Term Frame Encoder computes interdependencies across a hierarchy of temporal scales by sampling sequences at increasing 
                            strides around the frame of interest. Furthermore, we employ a long-term Transformer encoder over the frame embeddings to further 
                            enhance long-term reasoning. MuST achieves higher performance than previous state-of-the-art methods on three different public 
                            benchmarks.</p>
                        <ul class="modal-popup__cat">
                            <li>Poster</li>
                            <li>MICCAI 2024</li>
                        </ul>
                    </div>
        
                    <a href="https://arxiv.org/abs/2407.17361" target="_blank" rel="noopener noreferrer" class="modal-popup__details">Paper</a>
                </div>
            </div> <!-- end modal -->
        </div>

        <div style="margin-bottom: -45pt;" class="row block-large-1-4 block-medium-1-1 block-tab-1-1 block-500-stack folio-list">

            <div class="column folio-item">
                <a href="#modal-07" class="folio-item__thumb">
                    <img src="images/GraSP.png" 
                         srcset="images/GraSP.png 1x, 
                                 images/GraSP.png 2x" 
                         alt=""
                         style="padding-top: 3%; padding-bottom: 3%;"
                                 >
                </a>
            </div> <!-- end folio-item -->

            <div class="column2">
                <p> 
                    <strong>Pixel-Wise Recognition for Holistic Surgical Scene Understanding.</strong> <br>
                    <u>Nicolás Ayobi</u>, Santiago Rodríguez, Alejandra Pérez, Isabela Hernández, Nicolás Aparicio, Eugenie Dessevres, Sebastián Peña, Jessica Santander, Juan Ignacio Caicedo, Nicolás Fernández, Pablo Arbeláez<br>
                    <em>In revision for Medical Image Analysis.</em><br>
                    
                    <a href="https://arxiv.org/abs/2401.11174" target="_blank" rel="noopener noreferrer">[PDF]</a>
                    <a href="https://github.com/BCV-Uniandes/GraSP" target="_blank" rel="noopener noreferrer">[Code]</a>
                    <a href="http://157.253.243.19/GraSP" target="_blank" rel="noopener noreferrer">[Data]</a>
                </p>
            </div> <!-- end folio-item -->


            <!-- Modal Templates Popup
            =========================================================== -->
            <div id="modal-07" hidden>
                <div class="modal-popup">
                    <img src="images/GraSP.png" alt="" />
        
                    <div class="modal-popup__desc">
                        <!--h5>Panoptic Narrative Grounding</h5-->
                        <p style="text-align: justify">This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated
                            benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity.
                            Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases
                            and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To
                            exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS)
                            model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument
                            segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the
                            impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity requirements of
                            each task, and establish TAPIS’s superiority over previously proposed baselines and conventional CNN-based models. Additionally,
                            we validate the robustness of our method across multiple public benchmarks, confirming the reliability and applicability of our
                            dataset. This work represents a significant step forward in Endoscopic Vision, offering a novel and comprehensive framework for
                            future research towards a holistic understanding of surgical procedures.</p>
                        <ul class="modal-popup__cat">
                            <li>Preprint</li>
                            <li>ArXiv</li>
                        </ul>
                    </div>
        
                    <a href="https://arxiv.org/abs/2401.11174" target="_blank" rel="noopener noreferrer" class="modal-popup__details">Paper</a>
                </div>
            </div> <!-- end modal -->
        </div>

        <div style="margin-bottom: -45pt;" class="row block-large-1-4 block-medium-1-1 block-tab-1-1 block-500-stack folio-list">

            <div class="column folio-item">
                <a href="#modal-06" class="folio-item__thumb">
                    <img src="images/STRIDE.png" 
                         srcset="images/STRIDE.png 1x, 
                                 images/STRIDE.png 2x" 
                         alt=""
                         style="padding-top: 3%; padding-bottom: 3%;"
                                 >
                </a>
            </div> <!-- end folio-item -->

            <div class="column2">
                <p> 
                    <strong>STRIDE: Street View-based Environmental Feature Detection and Pedestrian Collision Prediction.</strong> <br>
                    Cristina González*, <u>Nicolás Ayobi*</u>, Felipe Escallón, Laura Baldovino-Chiquillo, Maria Wilches-Mogollón, Donny Pasos, Nicole Ramírez, Jose Pinzón, Olga Sarmiento, D. Alex Quistberg, Pablo Arbeláez. <br>
                    <em>Oral presentation at the second ROAD++ workshop hosted at the International Conference of Computer Vision (ICCVW) 2023. <u>Awarded Best Student Paper.</u></em><br>
                    
                    <a href="https://arxiv.org/abs/2308.13183" target="_blank" rel="noopener noreferrer">[PDF]</a>
                    <a href="https://ieeexplore.ieee.org/document/10350902" target="_blank" rel="noopener noreferrer">[Paper]</a>
                    <a href="https://github.com/BCV-Uniandes/STRIDE" target="_blank" rel="noopener noreferrer">[Code]</a>
                    <a href="http://157.253.243.19/STRIDE/" target="_blank" rel="noopener noreferrer">[Data]</a>
                </p>
            </div> <!-- end folio-item -->


            <!-- Modal Templates Popup
            =========================================================== -->
            <div id="modal-06" hidden>
                <div class="modal-popup">
                    <img src="images/STRIDE.png" alt="" />
        
                    <div class="modal-popup__desc">
                        <!--h5>Panoptic Narrative Grounding</h5-->
                        <p style="text-align: justify">This paper introduces a novel benchmark to study the impact and relationship of built environment elements on pedestrian collision prediction, intending to enhance environmental awareness in autonomous driving systems to prevent pedestrian injuries actively. We introduce a built environment detection task in large-scale panoramic images and a detection-based pedestrian collision frequency prediction task. We propose a baseline method that incorporates a collision prediction module into a state-of-the-art detection model to tackle both tasks simultaneously. Our experiments demonstrate a significant correlation between object detection of built environment elements and pedestrian collision frequency prediction. Our results are a stepping stone towards understanding the interdependencies between built environment conditions and pedestrian safety.</p>
                        <ul class="modal-popup__cat">
                            <li>Poster</li>
                            <li>ICCVW 2023</li>
                        </ul>
                    </div>
        
                    <a href="https://arxiv.org/abs/2308.13183" target="_blank" rel="noopener noreferrer" class="modal-popup__details">Paper</a>
                </div>
            </div> <!-- end modal -->
        </div>

        <div style="margin-bottom: -45pt;" class="row block-large-1-4 block-medium-1-1 block-tab-1-1 block-500-stack folio-list">

            <div class="column folio-item">
                <a href="#modal-05" class="folio-item__thumb">
                    <img src="images/PiGLET.png" 
                         srcset="images/PiGLET.png 1x, 
                                 images/PiGLET.png 2x" 
                         alt=""
                         style="padding-top: 3%; padding-bottom: 3%;"
                                 >
                </a>
            </div> <!-- end folio-item -->

            <div class="column2">
                <p> 
                    <strong>PiGLET: Pixel-level Grounding of Language Expressions with Transformers.</strong> <br>
                    Cristina González, <u>Nicolás Ayobi</u>, Isabela Hernández, Jordi Pont-Tuset, Pablo Arbeláez. <br>
                    <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2023.</em><br>
                    
                    <a href="https://bcv-uniandes.github.io/panoptic-narrative-grounding/" target="_blank" rel="noopener noreferrer">[Project Page]</a>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10158238" target="_blank" rel="noopener noreferrer">[Paper]</a>
                    <a href="https://bcv-uniandes.github.io/panoptic-narrative-grounding/" target="_blank" rel="noopener noreferrer">[Code]</a>
                    <a href="https://bcv-uniandes.github.io/panoptic-narrative-grounding/#downloads" target="_blank" rel="noopener noreferrer">[Data]</a>
                </p>
            </div> <!-- end folio-item -->


            <!-- Modal Templates Popup
            =========================================================== -->
            <div id="modal-05" hidden>
                <div class="modal-popup">
                    <img src="images/PiGLET.png" alt="" />
        
                    <div class="modal-popup__desc">
                        <!--h5>Panoptic Narrative Grounding</h5-->
                        <p style="text-align: justify">This paper proposes Panoptic Narrative Grounding, a spatially fine and general formulation of the natural language visual grounding problem. We establish an experimental framework for the study of this new task, including new ground truth and metrics. We propose PiGLET a novel multi-modal Transformer architecture to tackle the Panoptic Narrative Grounding task, and to serve as a stepping stone for future work. We exploit the intrinsic semantic richness in an image by including panoptic categories, and we approach visual grounding at a fine-grained level using segmentations. In terms of ground truth, we propose an algorithm to automatically transfer Localized Narratives annotations to specific regions in the panoptic segmentations of the MS COCO dataset. PiGLET achieves a performance of 63.2 absolute Average Recall points. By leveraging the rich language information on the Panoptic Narrative Grounding benchmark on MS COCO, PiGLET obtains an improvement of 0.4 Panoptic Quality points over its base method on the panoptic segmentation task. Finally, we demonstrate the generalizability of our method to other natural language visual grounding problems such as Referring Expression Segmentation. PiGLET is competitive with previous state-of-the-art in RefCOCO, RefCOCO+, and RefCOCOg.</p>
                        <ul class="modal-popup__cat">
                            <li>Article</li>
                            <li>TPAMI 2023</li>
                        </ul>
                    </div>
        
                    <a href="https://bcv-uniandes.github.io/panoptic-narrative-grounding/" target="_blank" rel="noopener noreferrer" class="modal-popup__details">Project Page</a>
                </div>
            </div> <!-- end modal -->

            
        </div>

        <div style="margin-bottom: -45pt;" class="row block-large-1-4 block-medium-1-1 block-tab-1-1 block-500-stack folio-list">

            <div class="column folio-item">
                <a href="#modal-04" class="folio-item__thumb">
                    <img src="images/MATIS_crop.png" 
                         srcset="images/MATIS_crop.png 1x, 
                                 images/MATIS_crop.png 2x" alt="">
                </a>
            </div> <!-- end folio-item -->

            <div class="column2">
                <p>
                    <strong>MATIS: Masked-Attention Transformers for Surgical Instrument Segmentation.</strong> <br>
                    <u>Nicolás Ayobi</u>, Alejandra Pérez-Rondón, Santiago Rodríguez, Pablo Arbeláez. <br>
                    <em>Oral presentation at the IEEE International Symposium on Biomedical Imaging (ISBI) 2023</em>.<br>
                    
                    <a href="https://arxiv.org/abs/2303.09514">[PDF]</a>
                    <a href="https://ieeexplore.ieee.org/document/10230819">[Paper]</a>
                    <a href="https://github.com/BCV-Uniandes/MATIS">[Code]</a>
                    <a href="http://157.253.243.19/MATIS/">[Data]</a>
                    
                </p> 
            </div> <!-- end folio-item -->

            <div id="modal-04" hidden>
                <div class="modal-popup">
                    <img src="images/MATIS_crop.png" alt="" />
                    
                    <div class="modal-popup__desc">
                        <p style="text-align: justify">
                            We propose Masked-Attention Transformers for Surgical Instrument Segmentation (MATIS), a two-stage, fully transformer-based method that leverages modern pixel-wise attention mechanisms for instrument segmentation. MATIS exploits the instance-level nature of the task by employing a masked attention module that generates and classifies a set of fine instrument region proposals. Our method incorporates long-term video-level information through video transformers to improve temporal consistency and enhance mask classification. We validate our approach in the two standard public benchmarks, Endovis 2017 and Endovis 2018. Our experiments demonstrate that MATIS’ per-frame baseline outperforms previous state-of-the-art methods and that including our temporal consistency module boosts our model’s performance further.</p>
                            <ul class="modal-popup__cat">
                                <li>Oral</li>
                                <li>ISBI 2023</li>
                            </ul>
                        </div>
            
                        <a href="https://ieeexplore.ieee.org/document/10230819" target="_blank" rel="noopener noreferrer" class="modal-popup__details">Paper</a>
                </div>
            </div> <!-- end modal -->
        </div>

        <div style="margin-bottom: -45pt;" class="row block-large-1-4 block-medium-1-1 block-tab-1-1 block-500-stack folio-list">

            <div class="column folio-item">
                <a href="#modal-03" class="folio-item__thumb">
                    <img src="images/TAPIR.jpg" 
                         srcset="images/TAPIR.jpg 1x, 
                                 images/TAPIR.jpg 2x" alt="">
                </a>
            </div> <!-- end folio-item -->

            <div class="column2">
                <p>
                    <strong>Towards Holistic Surgical Scene Understanding.</strong> <br>
                    Natalia Valderrama, Paola Ruíz, Isabela Hernández, <u>Nicolás Ayobi</u>, Mathilde Verlyk, Jessica Santander, Juan Caicedo, Nicolás Fernández, Pablo Arbeláez. <br>
                    <em>Oral Presentation at the International Conference on Medican Image Computing and Computer Assisted Interventions (MICCAI) 2022. <u>Nominated for Best Paper Award.</u></em><br>
                    
                    <a href="https://cinfonia.uniandes.edu.co/publications/towards-holistic-surgical-scene-understanding/">[Project Page]</a>
                    <a href="https://arxiv.org/abs/2212.04582">[PDF]</a>
                    <a href="https://link.springer.com/chapter/10.1007/978-3-031-16449-1_42">[Paper]</a>
                    <a href="https://github.com/BCV-Uniandes/TAPIR">[Code]</a>
                    <a href="http://157.253.243.19/PSI-AVA/">[Data]</a>
                </p>
                
            </div> <!-- end folio-item -->

            <div id="modal-03" hidden>
                <div class="modal-popup">
                    <img src="images/TAPIR.jpg" alt="" />
                    
                    <div class="modal-popup__desc">
                        <!--h5>Towards Holistic Surgical Scene Understanding</h5-->
                        <p style="text-align: justify">
                            Most benchmarks for studying surgical interventions focus on a specific challenge instead of leveraging the intrinsic complementarity among different tasks. In this work, we present a new experimental framework towards holistic surgical scene understanding. First, we introduce the Phase, Step, Instrument, and Atomic Visual Action recognition (PSIAVA) Dataset. PSI-AVA includes annotations for both long-term (Phase and Step recognition) and short-term reasoning (Instrument detection and novel Atomic Action recognition) in robot-assisted radical prostatectomy videos. Second, we present Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR) as a strong baseline for surgical scene understanding. TAPIR leverages our dataset’s multi-level annotations as it benefits from the learned representation on the instrument detection task to improve its classification capacity. Our experimental results in both PSI-AVA and other publicly available databases demonstrate the adequacy of our framework to spur future research on holistic surgical scene understanding.
                        </p>
                        <ul class="modal-popup__cat">
                            <li>Oral</li>
                            <li>MICCAI 2022</li>
                        </ul>
                        </div>
            
                        <a href="https://link.springer.com/chapter/10.1007/978-3-031-16449-1_42" target="_blank" rel="noopener noreferrer" class="modal-popup__details">Paper</a>
                </div>
            </div> <!-- end modal -->
        </div>

        <div style="margin-bottom: -45pt;" class="row block-large-1-4 block-medium-1-1 block-tab-1-1 block-500-stack folio-list">

            <div class="column folio-item">
                <a href="#modal-02" class="folio-item__thumb">
                    <img src="images/PNG.gif" 
                         srcset="images/PNG.gif 1x, 
                                 images/PNG.gif 2x" 
                         alt=""
                         style="padding-top: 3%; padding-bottom: 3%;"
                                 >
                </a>
            </div> <!-- end folio-item -->

            <div class="column2">
                <p> 
                    <strong>Panoptic Narrative Grounding.</strong> <br>
                    Cristina González, <u>Nicolás Ayobi</u>, Isabela Hernández, José Hernández, Jordi Pont-Tuset, Pablo Arbeláez. <br>
                    <em>Oral presentation at the International Conference of Computer Vision (ICCV) 2021.</em><br>
                    
                    <a href="https://bcv-uniandes.github.io/panoptic-narrative-grounding/" target="_blank" rel="noopener noreferrer">[Project Page]</a>
                    <a href="https://ieeexplore.ieee.org/document/9710546" target="_blank" rel="noopener noreferrer">[Paper]</a>
                    <a href="https://arxiv.org/abs/2109.04988" target="_blank" rel="noopener noreferrer">[PDF]</a>
                    <a href="https://bcv-uniandes.github.io/panoptic-narrative-grounding/" target="_blank" rel="noopener noreferrer">[Code]</a>
                    <a href="https://bcv-uniandes.github.io/panoptic-narrative-grounding/#downloads" target="_blank" rel="noopener noreferrer">[Data]</a>
                </p>
            </div> <!-- end folio-item -->


            <!-- Modal Templates Popup
            =========================================================== -->
            <div id="modal-02" hidden>
                <div class="modal-popup">
                    <img src="images/PNG.gif" alt="" />
        
                    <div class="modal-popup__desc">
                        <!--h5>Panoptic Narrative Grounding</h5-->
                        <p style="text-align: justify">This paper proposes Panoptic Narrative Grounding, a spatially fine and general 
                            formulation of the natural language visual grounding problem. We establish an 
                            experimental framework for the study of this new task, including new ground 
                            truth and metrics, and we propose a strong baseline method to serve as stepping 
                            stone for future work. We exploit the intrinsic semantic richness in an image 
                            by including panoptic categories, and we approach visual grounding at a 
                            fine-grained level by using segmentations. In terms of ground truth, we propose 
                            an algorithm to automatically transfer Localized Narratives annotations to 
                            specific regions in the panoptic segmentations of the MS COCO dataset. To 
                            guarantee the quality of our annotations, we take advantage of the semantic 
                            structure contained in WordNet to exclusively incorporate noun phrases that are 
                            grounded to a meaningfully related panoptic segmentation region. The proposed 
                            baseline achieves a performance of 55.4 absolute Average Recall points. This 
                            result is a suitable foundation to push the envelope further in the development 
                            of methods for Panoptic Narrative Grounding.</p>
                        <ul class="modal-popup__cat">
                            <li>Oral</li>
                            <li>ICCV 2021</li>
                        </ul>
                    </div>
        
                    <a href="https://bcv-uniandes.github.io/panoptic-narrative-grounding/" target="_blank" rel="noopener noreferrer" class="modal-popup__details">Project Page</a>
                </div>
            </div> <!-- end modal -->

            
        </div>

        <div style="margin-bottom: -45pt;" class="row block-large-1-4 block-medium-1-1 block-tab-1-1 block-500-stack folio-list">

            <div class="column folio-item">
                <a href="#modal-01" class="folio-item__thumb">
                    <img src="images/Javier.png" 
                         srcset="images/Javier.png 1x, 
                                 images/Javier.png 2x" alt="">
                </a>
            </div> <!-- end folio-item -->

            <div class="column2">
                <p>
                    <strong>Development of a Movil App for the Preoperative Evaluation of Sinus CT Scan: One Step Towards Artificial Intelligence.</strong> <br>
                    Javier Ospina, Cristhian Forigua, Andrés Hernández, <u>Nicolás Ayobi</u>, Tomás Correa, Augusto Peñaranda, Arif Janjua. <br>
                    <em>Oral presentation at Acta de Otorrinolaringología y Cirugía de Cabeza y Cuello 2022</em>.<br>
                    
                    <a href="https://www.medigraphic.com/cgi-bin/new/resumenI.cgi?IDARTICULO=107230">[Paper]</a>
                </p> 
            </div> <!-- end folio-item -->

            <div id="modal-01" hidden>
                <div class="modal-popup">
                    <img src="images/Javier.png" alt="" />
                    
                    <div class="modal-popup__desc">
                        <!--h5>Towards Holistic Surgical Scene Understanding</h5-->
                        <p style="text-align: justify">
                            The recent technology revolution that we have experienced has generated extensive interest in the use of artificial intelligence (AI) in the development of various systems and solutions in medicine. In the field of Otorhinolaryngology, we are seeing the first efforts to take advantage of this flourishing area. Objective: We sought to describe the development process of a mobile app created through a collaborative effort between ENT surgeons and biomedical engineers. This app has the intention to optimize the preoperative evaluation of paranasal sinus tomography (CT) to improve safety and outcomes in Endoscopic Sinus Surgery (ESS). Methods: The development of the app followed the prioritization method for MoSCoW specifications. We used the information collected from surveys of 29 Rhinology experts from different parts of the world, who evaluated anatomical variants on sinus CT scans. Two regression models were used to predict difficulty and risk using statistical learning. Conclusion: Via statistical modelling, we have developed a user-friendly tool that will ideally help surgeons assess the risk and difficulty of ESS based on the pre-operative CT scan of the sinuses. This is an exercise that demonstrates the efficacy of the collaborative efforts between surgeons and engineers to leverage AI tools and promote better solutions for our patients.</p>
                            <ul class="modal-popup__cat">
                                <li>Oral</li>
                                <li>Acta de Otorrinolaringología y Cirugía de Cabeza y Cuello 2022</li>
                            </ul>
                        </div>
            
                        <a href="https://www.medigraphic.com/cgi-bin/new/resumenI.cgi?IDARTICULO=107230" target="_blank" rel="noopener noreferrer" class="modal-popup__details">Paper</a>
                </div>
            </div> <!-- end modal -->
        </div>
    </section> <!-- end s-portfolio -->


    <!-- resume
    ================================================== -->
    <section id="resume" class="s-resume target-section">
        
        <div class="row s-resume__section">
            <div class="column large-3 tab-12">
                <h3 class="section-header-allcaps">Experience</h3>
            </div>
            <div class="column large-9 tab-12">
                <h6>Industry Experience<hr style="margin-top: 0%; margin-bottom: 5%;"></h6>
                <div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">Freelance AI Consultant and Developer</h4>
                        <p class="resume-block__header-meta">
                            <span>Plibotts</span> 
                            <span class="resume-block__header-date">
                                June 2023 - August 2023
                            </span>
                        </p>
                    </div>

                </div>
                
                <h6 style="margin-top: 5rem;">Research Experience<hr style="margin-top: 0%; margin-bottom: 5%;"></h6>
                <!-- <h6>Research Experience<hr style="margin-top: 0%; margin-bottom: 5%;"></h6> -->
                <div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">Research Assistant in Computer Vision and AI</h4>
                        <p class="resume-block__header-meta">
                            <span>CinfonIA Research Center</span> 
                            <span class="resume-block__header-date">
                                January 2022 - August 2024
                            </span>
                        </p>
                    </div>

                </div> <!-- end resume-block -->

                <div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">Undergraduate Research Assistant</h4>
                        <p class="resume-block__header-meta">
                            <span>CinfonIA Research Center</span> 
                            <span class="resume-block__header-date">
                                June 2020 - December 2021
                            </span>
                        </p>
                    </div>

                </div> <!-- end resume-block -->

                <h6 style="margin-top: 5rem;">Teaching Experience<hr style="margin-top: 0%; margin-bottom: 5%;"></h6>

                <div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">Graduate Teaching Assistant in Computer Vision</h4>
                        <p class="resume-block__header-meta">
                            <span>Universidad de los Andes (Bogotá, Colombia)</span> 
                        </p>
                    </div>

                    <p>
                        <span class="resume-block__header-utadate">
                            June 2023 - August 2023
                            </span> ~ Research Summer School of Introduction to Deep Learning <br>

                        <span class="resume-block__header-utadate">
                            August 2022 - December 2022
                            </span> ~ Advanced Machine Learning <br>

                        <span class="resume-block__header-utadate">
                            January 2022 - June 2022
                            </span> ~ Computer Vision <br>

                        <span class="resume-block__header-utadate">
                            January 2022 - September 2022
                            </span> ~ Coursera's Master's Degree in AI by Uniandes <br>
                        
                    </p>

                </div>

                <div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">Undergraduate Teaching Assistant</h4>
                        <p class="resume-block__header-meta">
                            <span>Universidad de los Andes (Bogotá, Colombia)</span> 
                        </p>
                    </div>

                    <p>
			        <span class="resume-block__header-utadate">
                                November 2021 - December 2021
                            </span> ~ Coursera's Master's Degree in AI by Uniandes <br>
			    
                    <span class="resume-block__header-utadate">
                                August 2021 - December 2021
                            </span> ~ Introduction to Programming <br>

					<span class="resume-block__header-utadate">
                                January 2021 - June 2021
                            </span> ~ Data Structures and Algorithms <br>
							
					<span class="resume-block__header-utadate">
                                August 2020 - June 2021
                            </span> ~ Data Structures <br>
					
					<span class="resume-block__header-utadate">
                                August 2020 - December 2020
                            </span> ~ Image Analysis and Processing <br>
					
					<span class="resume-block__header-utadate">
                                January 2020 - June 2020
                            </span> ~ Biomedical Engineering Foundations for Neurosurgery <br>
                    </p>

                </div> <!-- end resume-block -->
            </div>
        </div> <!-- s-resume__section -->
        </section>

        <hr>

        <section id="education" class="s-resume target-section">
        <div class="row s-resume__section">
            <div class="column large-3 tab-12">
                <h3 class="section-header-allcaps">Education</h3>
            </div>
            <div class="column large-9 tab-12">

                <div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">M.Sc. Biomedical Engineering</h4>
                        <p class="resume-block__header-meta">
                            <span>Universidad de los Andes (Bogotá, Colombia)</span> 
                            <span class="resume-block__header-date">
                                January 2022 - December 2023
                            </span>
                        </p>
                    </div>

                    <!-- <p>
                    GPA: 4.75/5.00</p>
                    <p>
                    Currently coursing the first year of my master's studies in biomedical engineering with a focus in computer vision for surgical workflow analysis and visual grounding. 
                    </p> -->

                </div>

                <div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">B.Sc. Biomedical Engineering (Cum Laude)</h4>
                        <p class="resume-block__header-meta">
                            <span>Universidad de los Andes (Bogotá, Colombia)</span> 
                            <span class="resume-block__header-date">
                                July 2017 - December 2021
                            </span>
                        </p>
                    </div>

                    <p>
                        <!-- <span class="resume-block__header-utadate">
                        </span> Cum Laude Honours <br> -->

                        <span class="resume-block__header-utadate">
                        </span> Double minor in Computational Mathematics and in Bioinformatics <br>
                    
                    </p>
                    <!-- <p>
                    - Double minor in computational mathematics and in bioinformatics
                    </p> -->

                </div> <!-- end resume-block -->

                <div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">Undergraduate Exchange Program in Biomedical Engineering</h4>
                        <p class="resume-block__header-meta">
                            <span>University of New South Wales (Sydney, Australia)</span> 
                            <span class="resume-block__header-date">
                                August 2019 - December 2019
                            </span>
                        </p>
                    </div>

                    <!-- <p>
                    I was awarded with my university's exchange program to take some courses 
                    in a foreign university. I traveled to the University of New South Wales 
                    in Sydney, Australia, were I learned Biological Signal Analysis, Tissue 
                    Engineering and Mechanincs of the Human Body from the outstanding 
                    researchers and teachers in this university.
                    </p> -->

                </div> <!-- end resume-block -->
				
				<div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">International Baccalaureate Diploma</h4>
                        <p class="resume-block__header-meta">
                            <span>Gimnasio Vermont School (Bogotá, Colombia)</span> 
                            <span class="resume-block__header-date">
                                August 2015 - June 2017
                            </span>
                        </p>
                    </div>

                    <!-- <p>
                    Score: 36/45</p>
                    <p>
                    I had my high school education at this institution. Here I obtained my 
                    International Baccalaureate (IB) diploma, achieving the highest grade in 
                    my graduation extended essay on helicopter flight optimization, and 
                    obtaining a score positioned as 4th of the class out of 32 students.
                    </p> -->

                </div> <!-- end resume-block -->
				
				<!--div class="resume-block">

                    <div class="resume-block__header">
                        <h4 class="h3">Gimnasio Vermont School (Bogotá, Colombia)</h4>
                        <p class="resume-block__header-meta">
                            <span>Basic Education</span> 
                            <span class="resume-block__header-date">
                                August 2004 - June 2017
                            </span>
                        </p>
                    </div>

                    <p>
                    Here I coursed my preeschool plus my elementary and middle education.
                    </p>

                </div--> <!-- end resume-block -->
            </div>
        </div> <!-- s-resume__section -->
        </section>

        <!--<section id="skills" class="s-resume target-section">
        <div class="row s-resume__section">
            <div class="column large-3 tab-12">
                <h3 class="section-header-allcaps">Skills</h3>
            </div>
            <div class="column large-9 tab-12">
                <div class="resume-block">

                    <ul class="skill-bars-fat">
                        <li>
                        <div class="progress percent100"></div>
                        <strong>Python</strong>
                        </li>
                        <li>
                        <div class="progress percent95"></div>
                        <strong>Pytorch</strong>
                        </li>
                        <li>
                        <div class="progress percent90"></div>
                        <strong>Matlab</strong>
                        </li>
                        <li>
                        <div class="progress percent90"></div>
                        <strong>Java</strong>
			            <li>
                        <div class="progress percent85"></div>
                        <strong>LaTeX</strong>
                        </li>
                        <li>
                        <div class="progress percent85"></div>
                        <strong>Bash</strong>
                        </li>
                        <li>
                        <div class="progress percent25"></div>
                        <strong>Docker</strong>
                        </li>
                        <li>
                        <div class="progress percent15"></div>
                        <strong>HTML</strong>
                        </li>
                        <li>
                        <div class="progress percent10"></div>
                        <strong>C++</strong>
                        </li>
                        <li>
                        <div class="progress percent10"></div>
                        <strong>SQL</strong>
                        </li>
                    </ul>

                    <h2>Language Skills:</h2>
                    <h1>Spanish (Native)</h1>
                    <h1>English (C1)</h1>
                   <h2>Soft Skills:</h2>
                    <p>
                    Self-taught, Quick Learner, Resourceful, Multi-Tasking, Teamworker, Persistence, Leadership
                    </p>


                </div> 

            </div>
        </div> 

    </section> end s-resume -->


    <!-- Extracurricular activities
    ================================================== -->
    <!--<section id="extracurricular" class="s-testimonials target-section">

        <div class="s-testimonials__bg"></div>

        <div class="row s-testimonials__header">
            <div class="column large-12">
                <h3>Extracurricular Activities</h3>
            </div>
        </div>

        <div class="row s-testimonials__content">

            <div class="column">

                <div class="swiper-container testimonial-slider">

                    <div class="swiper-wrapper">

                        <div class="testimonial-slider__slide swiper-slide">
                            <div class="testimonial-slider__author">
                                <img src="images/Volleyball.jpeg" class="testimonial-slider__avatar">
                            </div>
                            <span>Volleyball</span>
                            <p>
                            I've been a volleyball for the last 8 years. I used to be the captain my school's team and I played for the regional team. I'm currently playing with the university team with whom I've been regional champion three times.
                            </p>
                        </div> <-- end testimonial-slider__slide
        
                        <div class="testimonial-slider__slide swiper-slide">
                            <span>Latin Dancing</span>
                            <p>
                            For the last 6 years I've practiced different latin dance styles in multiple modalities. I've performed in different scenarios and events, and I've competed in several contests.
                            </p>
                            <div class="testimonial-slider__author">
                                <img src="images/Dance.PNG" alt="Author image" class="testimonial-slider__avatar">
                            </div>
                        </div> <-- end testimonial-slider__slide 
    
                    </div> <-- end testimonial slider swiper-wrapper 

                    <div class="swiper-pagination"></div>

                </div> <-- end swiper-container 

            </div> <-- end column 

        </div> <-- end row 

    </section> <-- end s-testimonials -->


    <!-- footer
    ================================================== -->
    <footer class="s-footer">
        <div class="row">
            <!--div class="column large-4 medium-6 w-1000-stack s-footer__social-block">
                <ul class="s-footer__social">
                    <li><a href="#0"><i class="fab fa-facebook-f" aria-hidden="true"></i></a></li>
                    <li><a href="#0"><i class="fab fa-twitter" aria-hidden="true"></i></a></li>
                    <li><a href="#0"><i class="fab fa-instagram" aria-hidden="true"></i></a></li>
                    <li><a href="#0"><i class="fab fa-dribbble" aria-hidden="true"></i></a></li>
                    <li><a href="#0"><i class="fab fa-linkedin-in" aria-hidden="true"></i></a></li>
                </ul>
            </div-->

            <div class="column large-7 medium-6 w-1000-stack ss-copyright">
                <span>© Copyright Ceevee 2021</span> 
                <span>Design by <a href="https://www.styleshout.com/">StyleShout</a></span>
            </div>
        </div>

        <div class="ss-go-top">
            <a class="smoothscroll" title="Back to Top" href="#top">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6 4h12v2H6zm5 10v6h2v-6h5l-6-6-6 6z"/></svg>
             </a>
        </div> <!-- end ss-go-top -->
    </footer>


    <!-- Java Script
    ================================================== -->
    <script src="js/plugins.js"></script>
    <script src="js/main.js"></script>

</body>

</html>
